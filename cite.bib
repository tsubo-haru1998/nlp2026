@article{magiv3,
  title={From Panels to Prose: Generating Literary Narratives from Comics},
  author={Sachdeva, Ragav and Zisserman, Andrew},
  journal={arXiv preprint arXiv:2503.23344},
  year={2025}
}

@article{mangaVQAandLMM,
  title={MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding},
  author={Baek, Jeonghun and Egashira, Kazuki and Onohara, Shota and Miyai, Atsuyuki and Imajuku, Yuki and Ikuta, Hikaru and Aizawa, Kiyoharu},
  journal={arXiv preprint arXiv:2505.20298},
  year={2025}
}

@InProceedings{ComicBERT,
author="Soykan, G{\"u}rkan
and Yuret, Deniz
and Sezgin, Tevfik Metin",
editor="Mouch{\`e}re, Harold
and Zhu, Anna",
title="ComicBERT: A Transformer Model and Pre-training Strategy for Contextual Understanding in Comics",
booktitle="Document Analysis and Recognition -- ICDAR 2024 Workshops",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="257--281",
abstract="Despite the growing interest in digital comic processing, foundational models tailored for this medium still need to be explored. Existing methods employ multimodal sequential models with cloze-style tasks, but they fall short of achieving human-like understanding. Addressing this gap, we introduce a novel transformer-based architecture, Comicsformer, and a comprehensive framework, ComicBERT, designed to process and understand the complex interplay of visual and textual elements in comics. Our approach utilizes a self-supervised objective, Masked Comic Modeling, inspired by BERT's [6] masked language modeling objective, to train the foundation model. To fine-tune and validate our models, we adopt existing cloze-style tasks and propose new tasks - such as scene-cloze, which better capture the narrative and contextual intricacies unique to comics. Preliminary experiments indicate that these tasks enhance the model's predictive accuracy and may provide new tools for comic creators, aiding in character dialogue generation and panel sequencing. Ultimately, ComicBERT aims to serve as a universal comic processor.",
isbn="978-3-031-70645-5"
}

@article{Transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{3DCNN,
  title={Learning spatiotemporal features with 3d convolutional networks},
  author={Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4489--4497},
  year={2015}
}

@inproceedings{ViViT,
  title={Vivit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6836--6846},
  year={2021}
}

@inproceedings{TimeSformer,
  title={Is space-time attention all you need for video understanding?},
  author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle={Icml},
  volume={2},
  number={3},
  pages={4},
  year={2021}
}

@inproceedings{VideoSwinTransformer,
  title={Video swin transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3202--3211},
  year={2022}
}

@ARTICLE{MangaUB,
  author={Ikuta, Hikaru and Wöhler, Leslie and Aizawa, Kiyoharu},
  journal={IEEE MultiMedia}, 
  title={MangaUB: A Manga Understanding Benchmark for Large Multimodal Models}, 
  year={2025},
  volume={32},
  number={2},
  pages={33-43},
  keywords={Benchmark testing;Annotations;Visualization;Image recognition;Meteorology;Computational modeling;Biological system modeling;Training;Text recognition;Data mining},
  doi={10.1109/MMUL.2025.3550451}}

@article{Manga109_Aizawa,
    author={Kiyoharu Aizawa and Azuma Fujimoto and Atsushi Otsubo and Toru Ogawa and Yusuke Matsui and Koki Tsubota and Hikaru Ikuta},
    title={Building a Manga Dataset ``Manga109'' with Annotations for Multimedia Applications},
    journal={IEEE MultiMedia},
    volume={27},
    number={2},
    pages={8--18},
    doi={10.1109/mmul.2020.2987895},
    year={2020}
}
@article{Manga109_Matsui,
	author={Yusuke Matsui and Kota Ito and Yuji Aramaki and Azuma Fujimoto and Toru Ogawa and Toshihiko Yamasaki and Kiyoharu Aizawa},
	title={Sketch-based Manga Retrieval using Manga109 Dataset},
	journal={Multimedia Tools and Applications},
	volume={76},
	number={20},
	pages={21811--21838},
	doi={10.1007/s11042-016-4020-z},
	year={2017}
}

@inproceedings{Manga109Dialog,
  title={Manga109Dialog: A Large-scale Dialogue Dataset for Comics Speaker Detection},
  author={Li, Yingxuan and Aizawa, Kiyoharu and Matsui, Yusuke},
  booktitle={Proceedings of the IEEE International Conference on Multimedia and Expo},
  year={2024}
}

@inproceedings{EfficientNet,
  title={Efficientnet: Rethinking model scaling for convolutional neural networks},
  author={Tan, Mingxing and Le, Quoc},
  booktitle={International conference on machine learning},
  pages={6105--6114},
  year={2019},
  organization={PMLR}
}

@article{CharReID,
  title={Identity-aware semi-supervised learning for comic character re-identification},
  author={Soykan, G{\"u}rkan and Yuret, Deniz and Sezgin, Tevfik Metin},
  journal={arXiv preprint arXiv:2308.09096},
  year={2023}
}

@article{SentenceTransformer,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{Distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}